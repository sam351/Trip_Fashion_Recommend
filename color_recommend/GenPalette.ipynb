{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_name = 'voyagefox_'\n",
    "file_path = f'../data/{account_name}.json'\n",
    "\n",
    "with open(file_path, 'r') as json_fp:\n",
    "    palette_dict = json.load(json_fp)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> original picture : voyagefox__055_3772Likes.png\n",
      ">>> back_cols : ([65, 67, 70], [92, 84, 81], [150, 122, 108])\n",
      ">>> fore_cols : ([36, 44, 49], [61, 62, 65])\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the loaded json\n",
    "for item in palette_dict.items():\n",
    "    file_name = item[0]\n",
    "    back_cols = tuple(item[1][0])\n",
    "    fore_cols = tuple(item[1][1])\n",
    "    \n",
    "    print(f'>>> original picture : {file_name}')\n",
    "    print(f'>>> back_cols : {back_cols}')\n",
    "    print(f'>>> fore_cols : {fore_cols}\\n')\n",
    "    break\n",
    "\n",
    "\n",
    "# visualize one sample pair\n",
    "back_cols_img = np.expand_dims(np.array(back_cols), 0)\n",
    "fore_cols_img = np.expand_dims(np.array(fore_cols), 0)\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.imshow(back_cols_img)\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.imshow(fore_cols_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use PIL Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "<PIL.Image.Image image mode=RGB size=1000x1 at 0x7FA36B6E0E48>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAABCAIAAADCYhNkAAAAGklEQVR4nO3BMQEAAADCoPVPbQ0PoAAAgHsDC7kAAfTKO28AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1000x1 at 0x7FA36B6E0E48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate new background - Image.new(mode,size,color)\n",
    "im=Image.new('RGB',(1000,1),(0,0,0))\n",
    "print(type(im))\n",
    "print(im)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAABCAIAAADCYhNkAAAALUlEQVR4nO3UwQkAAAzCwLr/0HYJpVByE/gIygPkibDQQFgosHU9AQ9p+CvkLdtWBgB8D/z0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1000x1 at 0x7FA36B6E0E48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colorize the background\n",
    "draw=ImageDraw.Draw(im)\n",
    "draw.rectangle((0*200, 0, 1*200, 1), fill=(255, 0, 0))\n",
    "draw.rectangle((1*200, 0, 2*200, 1), fill=(0, 255, 0))\n",
    "draw.rectangle((2*200, 0, 3*200, 1), fill=(0, 0, 255))\n",
    "draw.rectangle((3*200, 0, 4*200, 1), fill=(255, 255, 0))\n",
    "draw.rectangle((4*200, 0, 5*200, 1), fill=(0, 255, 255))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAABCAIAAADCYhNkAAAAOUlEQVR4nGN0dHZjGAWjgNpAWpB7oJ0wCoYhsFKTHmgnjIJhCPo2HR1oJ4yCYQgkhQQG2gmjYBgCALp6A75NcVg0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1000x1 at 0x7FA36B6E0E48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colorize the background - using each palette in dataset\n",
    "draw=ImageDraw.Draw(im)\n",
    "batch = 200\n",
    "for j, c in enumerate(back_cols + fore_cols):\n",
    "    c = tuple(c)\n",
    "    draw.rectangle((j*batch,0,(j+1)*batch,1),fill=c)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading function, generator for training \n",
    "\n",
    "During the generation process, the original color scheme is used as the label. The foreground colors will be concealed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataset(account_name, output_x_num=3, output_y_num=2):  # x == back_cols, y == fore_cols\n",
    "    file_path = f'../data/{account_name}.json'\n",
    "    with open(file_path, 'r') as json_fp:\n",
    "        palette_dict = json.load(json_fp)\n",
    "    print(f'>>> finished loading {len(palette_dict)} palettes')\n",
    "    \n",
    "    dataAB = []\n",
    "    for back_cols, fore_cols in palette_dict.values():\n",
    "        # select only given number of colors\n",
    "        back_cols = back_cols[:output_x_num]\n",
    "        fore_cols = fore_cols[:output_y_num]\n",
    "        \n",
    "        # generate a new image - Image.new(mode,size,color)\n",
    "        im_size = (1000, 1)\n",
    "        tmp_im = Image.new('RGB', im_size, (0,0,0))\n",
    "        \n",
    "        # colorize the image - using background & forground colors\n",
    "        draw = ImageDraw.Draw(tmp_im)\n",
    "        batch = int( im_size[0] / (output_x_num + output_y_num) )\n",
    "        for idx, color in enumerate(back_cols + fore_cols):\n",
    "            color = tuple(color)  # due to PIL datatype requirement\n",
    "            draw.rectangle( xy=(idx*batch, 0, (idx+1)*batch, im_size[1]), fill=color )\n",
    "        \n",
    "        # append the colorized image\n",
    "        dataAB.append(tmp_im)\n",
    "        \n",
    "    # random.shuffle(dataAB)  # shuffle dataset before train-validation-split\n",
    "    trainAB = dataAB[ :int(len(dataAB) * 0.85) ]\n",
    "    valAB = dataAB[ int(len(dataAB) * 0.85): ]\n",
    "    return trainAB, valAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hide_y_cols(img_obj, x_num=3, y_num=2):\n",
    "    img_fake = img_obj.copy()\n",
    "    \n",
    "    draw_fake = ImageDraw.Draw(img_fake)\n",
    "    batch = int(img_fake.size[0] / (x_num+y_num))\n",
    "    for index in range(x_num, x_num+y_num):\n",
    "        draw_fake.rectangle( xy=(index*batch, 0, (index+1)*batch, img_fake.size[1]), fill=(0,0,0) )\n",
    "    \n",
    "    img_fake_arr = np.array(img_fake.resize((256,1))) / 255 * 2 - 1\n",
    "    img_real_arr = np.array(img_obj.resize((256,1))) / 255 * 2 - 1\n",
    "    return img_fake_arr, img_real_arr\n",
    "\n",
    "\n",
    "def minibatch(dataAB, batchsize, input_x_num=3, input_y_num=2):\n",
    "    length = len(dataAB) * 10\n",
    "    epoch = 0\n",
    "    \n",
    "    tmp_start = 0\n",
    "    tmp_size = None\n",
    "    while True:\n",
    "        size = tmp_size if tmp_size else batchsize\n",
    "        \n",
    "        # increase epoch\n",
    "        if (tmp_start + size) > length:\n",
    "            # random.shuffle(dataAB)\n",
    "            tmp_start = 0\n",
    "            epoch += 1\n",
    "        \n",
    "        dataA = []\n",
    "        dataB = []\n",
    "        for idx in range(tmp_start, tmp_start + size):\n",
    "            idx = int(idx % len(dataAB))\n",
    "            img_fake_arr, img_real_arr = hide_y_cols( dataAB[idx], input_x_num, input_y_num )\n",
    "            dataA.append(img_fake_arr)\n",
    "            dataB.append(img_real_arr)\n",
    "        \n",
    "        dataA = np.float32(dataA)\n",
    "        dataB = np.float32(dataB)\n",
    "        tmp_start += size\n",
    "        \n",
    "        tmp_size = yield epoch, dataA, dataB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showX(X, epoch=0, rows=3, savefig=True):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    X = np.concatenate(X,1)\n",
    "    for i in range(rows):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.imshow(np.array(Image.fromarray(((X[i]+1)/2*255).clip(0,255).astype('uint8')).resize((512,100))))\n",
    "    \n",
    "    if savefig:\n",
    "        if not os.path.exists('results/'):\n",
    "            os.makedirs('results/')\n",
    "        plt.savefig(f'results/epoch_{epoch}.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> finished loading 32 palettes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainAB, valAB = json_to_dataset(account_name)\n",
    "\n",
    "train_batch = minibatch(trainAB, batchsize=6)\n",
    "_, trainA, trainB = next(train_batch)\n",
    "\n",
    "showX([trainA, trainB], savefig=False)\n",
    "\n",
    "del train_batch, trainA, trainB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models built \n",
    "Pix2pix's network structure is used for training. Some differences are in the input size. I use 256 1 instead of 256 256 in the image . The input becomes smaller, so I increased the number of channels. By default, the first layer uses 128. Channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, Dropout\n",
    "from keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "from keras.initializers import RandomNormal\n",
    "import keras.backend as K\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import SVG\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic discriminator\n",
    "conv_init = RandomNormal(0, 0.02)\n",
    "gamma_init = RandomNormal(1., 0.02) # for batch normalization\n",
    "\n",
    "\n",
    "def conv2d(f, *a, **k):\n",
    "    return Conv2D(f, kernel_initializer = conv_init, *a, **k)\n",
    "def batchnorm():\n",
    "    return BatchNormalization(momentum=0.9, axis=-1, epsilon=1.01e-5,\n",
    "                                   gamma_initializer = gamma_init)\n",
    "def BASIC_D(nc_in=3, nc_out=3, ndf=128, max_layers=3):\n",
    "    \"\"\"DCGAN_D(nc, ndf, max_layers=3)\n",
    "       nc: channels\n",
    "       ndf: filters of the first layer\n",
    "       max_layers: max hidden layers\n",
    "    \"\"\"    \n",
    "\n",
    "    input_a, input_b = Input(shape=(None, None, nc_in)), Input(shape=(None, None, nc_out))\n",
    "    _ = Concatenate(axis=-1)([input_a, input_b])\n",
    "    _ = conv2d(ndf, kernel_size=(1,4), strides=(1,2), padding=\"same\", name = 'First') (_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    for layer in range(1, max_layers):        \n",
    "        out_feat = ndf * min(2**layer, 8)\n",
    "        _ = conv2d(out_feat, kernel_size=(1,4), strides=(1,2), padding=\"same\", \n",
    "                   use_bias=False, name = f'pyramid.{layer}'\n",
    "                        ) (_)\n",
    "        _ = batchnorm()(_, training=1)        \n",
    "        _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    out_feat = ndf*min(2**max_layers, 8)\n",
    "    _ = ZeroPadding2D((0,1))(_)\n",
    "    _ = conv2d(out_feat, kernel_size=(1,4),  use_bias=False, name = 'pyramid_last') (_)\n",
    "    _ = batchnorm()(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    # final layer\n",
    "    _ = ZeroPadding2D((0,1))(_)\n",
    "    _ = conv2d(1, kernel_size=(1,4), name = 'final'.format(out_feat, 1), \n",
    "               activation = \"sigmoid\") (_)    \n",
    "    return Model(inputs=[input_a, input_b], outputs=_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNET_G(isize, nc_in=3, nc_out=3, ngf=128, fixed_input_size=True):\n",
    "    def block(x, s, nf_in, use_batchnorm=True, nf_out=None, nf_next=None, max_nf=8*ngf):\n",
    "        # print(\"block\",x,s,nf_in, use_batchnorm, nf_out, nf_next)\n",
    "        assert s>=2 and s%2==0\n",
    "        if nf_next is None:\n",
    "            nf_next = min(nf_in*2, max_nf)\n",
    "        if nf_out is None:\n",
    "            nf_out = nf_in\n",
    "        x = conv2d(nf_next, kernel_size=(1,4), strides=(1,2), use_bias=(not (use_batchnorm and s>2)),\n",
    "                   padding=\"same\", name = f'conv_{s}') (x)\n",
    "        if s>2:\n",
    "            if use_batchnorm:\n",
    "                x = batchnorm()(x, training=1)\n",
    "            x2 = LeakyReLU(alpha=0.2)(x)\n",
    "            x2 = block(x2, s//2, nf_next)\n",
    "            x = Concatenate(axis=-1)([x, x2])            \n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2DTranspose(nf_out, kernel_size=(1,4), strides=(1,2), use_bias=not use_batchnorm,\n",
    "                            kernel_initializer = conv_init,          \n",
    "                            name = f'convt.{s}')(x)        \n",
    "        x = Cropping2D(cropping=((0,0),(1,1)))(x)\n",
    "        if use_batchnorm:\n",
    "            x = batchnorm()(x, training=1)\n",
    "        if s <=8:\n",
    "            x = Dropout(0.5)(x, training=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    s = isize if fixed_input_size else None\n",
    "    \n",
    "    inputs = Input(shape=(1, s, nc_in))        \n",
    "    _ = block(inputs, s=isize, nf_in=nc_in, use_batchnorm=False, nf_out=nc_out, nf_next=ngf)\n",
    "    _ = Activation('tanh')(_)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 6 0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "First (Conv2D)                  (None, None, None, 1 3200        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 1 0           First[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pyramid.1 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 2 1024        pyramid.1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pyramid.2 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 5 2048        pyramid.2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, None, None, 5 0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pyramid_last (Conv2D)           (None, None, None, 1 2097152     zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 1 4096        pyramid_last[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, None, None, 1 0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final (Conv2D)                  (None, None, None, 1 4097        zero_padding2d_2[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 2,766,977\n",
      "Trainable params: 2,763,393\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1, 256, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_256 (Conv2D)               (None, 1, 128, 128)  1664        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 1, 128, 128)  0           conv_256[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_128 (Conv2D)               (None, 1, 64, 256)   131072      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 64, 256)   1024        conv_128[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 1, 64, 256)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_64 (Conv2D)                (None, 1, 32, 512)   524288      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 32, 512)   2048        conv_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 1, 32, 512)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_32 (Conv2D)                (None, 1, 16, 1024)  2097152     leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1, 16, 1024)  4096        conv_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 1, 16, 1024)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 1, 8, 1024)   4194304     leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 1, 8, 1024)   4096        conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 1, 8, 1024)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, 1, 4, 1024)   4194304     leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1, 4, 1024)   4096        conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 1, 4, 1024)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 1, 2, 1024)   4194304     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 2, 1024)   4096        conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 1, 2, 1024)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 1, 1, 1024)   4195328     leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1, 1, 1024)   0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "convt.2 (Conv2DTranspose)       (None, 1, 4, 1024)   4194304     activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 1, 2, 1024)   0           convt.2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1, 2, 1024)   4096        cropping2d_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 2, 1024)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 2, 2048)   0           batch_normalization_9[0][0]      \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1, 2, 2048)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.4 (Conv2DTranspose)       (None, 1, 6, 1024)   8388608     activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)       (None, 1, 4, 1024)   0           convt.4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 1, 4, 1024)   4096        cropping2d_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 4, 1024)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 4, 2048)   0           batch_normalization_8[0][0]      \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1, 4, 2048)   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.8 (Conv2DTranspose)       (None, 1, 10, 1024)  8388608     activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)       (None, 1, 8, 1024)   0           convt.8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1, 8, 1024)   4096        cropping2d_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 8, 1024)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 8, 2048)   0           batch_normalization_7[0][0]      \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1, 8, 2048)   0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.16 (Conv2DTranspose)      (None, 1, 18, 1024)  8388608     activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 1, 16, 1024)  0           convt.16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1, 16, 1024)  4096        cropping2d_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1, 16, 2048)  0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1, 16, 2048)  0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.32 (Conv2DTranspose)      (None, 1, 34, 512)   4194304     activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_5 (Cropping2D)       (None, 1, 32, 512)   0           convt.32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1, 32, 512)   2048        cropping2d_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1, 32, 1024)  0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1, 32, 1024)  0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.64 (Conv2DTranspose)      (None, 1, 66, 256)   1048576     activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_6 (Cropping2D)       (None, 1, 64, 256)   0           convt.64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 1, 64, 256)   1024        cropping2d_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1, 64, 512)   0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1, 64, 512)   0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.128 (Conv2DTranspose)     (None, 1, 130, 128)  262144      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_7 (Cropping2D)       (None, 1, 128, 128)  0           convt.128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 1, 128, 128)  512         cropping2d_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1, 128, 256)  0           conv_256[0][0]                   \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1, 128, 256)  0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "convt.256 (Conv2DTranspose)     (None, 1, 258, 3)    3075        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_8 (Cropping2D)       (None, 1, 256, 3)    0           convt.256[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1, 256, 3)    0           cropping2d_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 54,440,067\n",
      "Trainable params: 54,420,355\n",
      "Non-trainable params: 19,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_filters = 128\n",
    "netD = BASIC_D(ndf=first_filters)\n",
    "\n",
    "imageSize = 256\n",
    "netG = UNET_G(isize=imageSize)\n",
    "\n",
    "netD.summary()\n",
    "netG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A = netG.input\n",
    "fake_B = netG.output\n",
    "netG_generate = K.function([real_A], [fake_B])\n",
    "\n",
    "real_B = netD.inputs[1]\n",
    "output_D_real = netD([real_A, real_B])\n",
    "output_D_fake = netD([real_A, fake_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = lambda output, target : -K.mean(K.log(output+1e-12)*target+K.log(1-output+1e-12)*(1-target))\n",
    "\n",
    "loss_D_real = loss_fn(output_D_real, K.ones_like(output_D_real))\n",
    "loss_D_fake = loss_fn(output_D_fake, K.zeros_like(output_D_fake))\n",
    "loss_G_fake = loss_fn(output_D_fake, K.ones_like(output_D_fake))\n",
    "\n",
    "loss_L1 = K.mean(K.abs(fake_B-real_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimizer\n",
    "lrD = 2e-4\n",
    "loss_D = loss_D_real +loss_D_fake\n",
    "training_updates = Adam(lr=lrD, beta_1=0.5).get_updates(netD.trainable_weights,[],loss_D)\n",
    "netD_train = K.function([real_A, real_B], [loss_D/2], training_updates)\n",
    "\n",
    "lrG = 2e-4\n",
    "loss_G = loss_G_fake   + 100 * loss_L1\n",
    "training_updates = Adam(lr=lrG, beta_1=0.5).get_updates(netG.trainable_weights,[], loss_G)\n",
    "netG_train = K.function([real_A, real_B], [loss_G_fake, loss_L1], training_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "During the training process, if the training is performed in the order of a generator and a discriminator, the result will be very poor, and the generation result is very bad, so the actual training one iteration took 3 times to generate a discriminant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netG_gen(A):\n",
    "    return np.concatenate([netG_generate([A[i:i+1]])[0] for i in range(A.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "batchSize = 1\n",
    "niter = 5\n",
    "display_iters = 2000\n",
    "\n",
    "gen_iterations = 0\n",
    "errL1 = epoch = errG = 0\n",
    "errL1_sum = errG_sum = errD_sum = 0\n",
    "\n",
    "train_batch = minibatch(trainAB, batchSize)\n",
    "val_batch = minibatch(valAB, 6)\n",
    "\n",
    "current_iter = 0\n",
    "while epoch < niter: \n",
    "    epoch, trainA, trainB = next(train_batch)        \n",
    "    errD,  = netD_train([trainA, trainB])\n",
    "    errD_sum += errD\n",
    "    \n",
    "    errG, errL1 = netG_train([trainA, trainB])\n",
    "    errG_sum += errG\n",
    "    errL1_sum += errL1\n",
    "    \n",
    "    for i in range(3):        \n",
    "        _, trainA1, trainB1 = next(train_batch)\n",
    "        errG, errL1 = netG_train([trainA1, trainB1])\n",
    "        errG_sum += errG\n",
    "        errL1_sum += errL1\n",
    "    \n",
    "    gen_iterations += 1\n",
    "    if gen_iterations % display_iters == 0:\n",
    "        if gen_iterations % (5*display_iters) == 0:\n",
    "            clear_output()\n",
    "        print(f'[{epoch}/{niter}][{gen_iterations}] Loss_D: {errD_sum/display_iters} ' + \n",
    "              f'Loss_G: {errG_sum/(display_iters*4)} loss_L1: {errL1_sum/(display_iters*4)}')\n",
    "        \n",
    "        _, valA, valB = train_batch.send(6) \n",
    "        fakeB = netG_gen(valA)\n",
    "        showX([valA, valB, fakeB], gen_iterations)\n",
    "        \n",
    "        errL1_sum = errG_sum = errD_sum = 0\n",
    "        _, valA, valB = next(val_batch)\n",
    "        fakeB = netG_gen(valA)\n",
    "        showX([valA, valB, fakeB], gen_iterations)\n",
    "        \n",
    "    if epoch > current_iter:\n",
    "        current_iter += 1\n",
    "        \n",
    "        if not os.path.exists('weights/100L1_128channel_4gen/generator/'):\n",
    "            os.makedirs('weights/100L1_128channel_4gen/generator/')\n",
    "        if not os.path.exists('weights/100L1_128channel_4gen/discriminator/'):\n",
    "            os.makedirs('weights/100L1_128channel_4gen/discriminator/')\n",
    "        \n",
    "        netG.save(f'./weights/100L1_128channel_4gen/generator/epoch_{epoch}.hdf5')\n",
    "        netD.save(f'./weights/100L1_128channel_4gen/discriminator/epoch_{epoch}.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pj2_sam/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# Here I trained three models based on different parameters, used here as appropriate\n",
    "# generator = load_model('weights/GAN/100L1_128channel_peisenet/generator/epoch_2.hdf5')\n",
    "# generator1 = load_model('weights/GAN/100L1_128channel_4gen/generator/epoch_10.hdf5')\n",
    "# generator2 = load_model('weights/GAN/1000L1_256channel/generator/epoch_10.hdf5')\n",
    "\n",
    "generator = load_model('weights/100L1_128channel_4gen/generator/epoch_3.hdf5')\n",
    "generator1 = load_model('weights/100L1_128channel_4gen/generator/epoch_4.hdf5')\n",
    "generator2 = load_model('weights/100L1_128channel_4gen/generator/epoch_5.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in generator.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 1 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACEklEQVR4nO3csWqTYRiG4SdJMRUEdRIdFASPIYODg6O4SEDEc3AR3QVxd5TiYHFREKWggyJ4ADp1tYW6FEVsUaw0afN7AlLwRb9SuK41w/sTHkhuAul1XRcAAAD+Tn+/HwAAAOAgElMAAAAFYgoAAKBATAEAABSIKQAAgIK5vV58/HK56V/9TWfJ+GzbvtvtelndGqbres1ubk+2c2V8MZ+/rDe7uV+6lm9sbPZ/mU0m+TC+lOnXzWY3k+Rd/3sGaTqhPJ1O2h5M8mTxRdvd7nS5PDqeWcOrO7NBNiZH2h1Msr66koe3bmYwt+dH3T81f3g+p8+daXYvSQaZ5vbzN013+2DhWdPNTnaS66O5ppvdzaFszU4kaXd049NaXt+7k/6g3Wb7R49l/vyFZveSZLb6PjcWl5puduHRUvPNXhsNm2/2Z+PN/tr8lpW3r9Lrt/su1BsO0z95qtm9JPnxcTlX797/42b9MgUAAFAgpgAAAArEFAAAQIGYAgAAKBBTAAAABWIKAACgQEwBAAAUiCkAAIACMQUAAFAgpgAAAArEFAAAQIGYAgAAKBBTAAAABWIKAACgQEwBAAAUiCkAAIACMQUAAFAgpgAAAArEFAAAQIGYAgAAKBBTAAAABWIKAACgoNd13X4/AwAAwIHjlykAAIACMQUAAFAgpgAAAArEFAAAQIGYAgAAKBBTAAAABb8B03pwQ2BQDsMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 2 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACMElEQVR4nO3cPWtTcRjG4ef0tDi4CV1sHP0UjoKTBakgFJ06CILQ6uKgIFLwFUTRqtBFwcnBrWMXFUHQVRDEl0YiioIoTWprcvwCUvBB/6FwXWuG+4Q8Q34EUjVNEwAAAPydkWE/AAAAwFYkpgAAABLEFAAAQIKYAgAASBBTAAAACaObvXj88N6if/W31luNa/eWS07Gj+/f4ujM/lhfXy+2ORg08eTZ61j7uVFsc1iapqlK7rnZ/2OkqmLPxO4YG6mLbUYVcX7pUWwbK7gZEZ87H4vebETEiSMHCt9tL+Yv3Ih+v19ss9Npx8LClajrcp/n+Ph4TB08FIPBoNhmr9OJN3cWi+1FRHzYsT1OLz0uerfTU5NFb7bb68bi9dtFb3Zl5V1cvHQ26nrTr0r/VKu1K+ZOnir6Pjc+tePL/avF9iIiXqxWMfuw7M3OTO4re7Pd1bh190H0+7+Kbbbfv43L585EPVruZne2JuLY3GzRm42NbsTXV+X2IuL505cxPX/zjzfrlykAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACVXTNMN+BgAAgC3HL1MAAAAJYgoAACBBTAEAACSIKQAAgAQxBQAAkCCmAAAAEn4DPGGBQy8NzVUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 3 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACCklEQVR4nO3cMWoUYRTA8TfrEoKFYicIElLYBiy9ibXgCRT0CskV7FwiqGgVRIMggkW8gggiaGEMYQ1JdHdnPi8gAR/6QuD3a6d43zKvmP8OTNdaCwAAAP7O6LQPAAAAcBaJKQAAgAQxBQAAkCCmAAAAEsQUAABAwvikixvvP5d+6m9oEWsXTzzSvzc7jNhej+gXdSMXfdxafxbfp4dlM09La62rnGdn/4/RuXHcuHk/xsvny2ZGRHzZvBPd0nLpzNXbD0p3NiLi0WSrdm+HFteuXIjKj7kOx9M4+vA2oqv7D29/dz+2H7+K0ahuZtdajIehbF5ExNLK5dh4vVO6tw8nL0t3th9aXF+p3dnFwW7s7TyNrnB/5rM+9r4eRFd4N4dFH79+1D6P7M2P4t6L2p2dPH9TvrNrVy+V7ux8uhvf3j0p3dn+53EcfvoYlUvbd6M4Gtc+G0xn47i7ufXHH+nNFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAICErrV22mcAAAA4c7yZAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJDwG8ywekMp3IYNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 4 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACI0lEQVR4nO3cvWoUYRSA4TMbUcIGQWIhGBDURtAijRcQLMULsLJWsLBK7yWkExsLA8aAnWJcsEiihQTBwk4ECwOrG4lRV5OdfN6ACB7wWwLP005x5uew8O7ANKWUAAAA4N90xn0CAAAAB5GYAgAASBBTAAAACWIKAAAgQUwBAAAkHPrbwfsf+1U/9TcqJS5NT9UcGcPd3VjoPYtR21abOdrbi8Wbt+LbYFBt5riUUpqa8+zs/zEatbF4+158/zGsNjMiounORFT+4mj79kHVnY2IeP5kvepFtvslZs90q97are2deLy6EZ1Ovdu7s7kZG3fvRDMxUW3m1NFuzF48V21eRMTPdj/ml1eq7u3L3qvKOxtx4fThqjv76cvXWF55EZ1Ovf+df33ux4dHS9FUnHn85Im4cuNatXkREe/XenF14WHdnV1/U/139vypyao729/ajqWna1V39shoGDODd1Gaeo9z8th0nJ27XG1eRET/9WrMXZ//40V6MwUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgoSmljPscAAAADhxvpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAk/AZwTIBDJauoOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing palette 5 ... \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAAkCAYAAABsd2B4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACIklEQVR4nO3cvWoVYRCA4dkQIolCFNJZK2ghWAgRm+QGks7CQrEQ1MJKsFCwES/BUkGwSaEgCHZ2FlaCogGJQcHmEFBBk+P5+7wBERxwQuB52i1m95xh4d2F7VprAQAAwL+Z2u0TAAAA2IvEFAAAQIKYAgAASBBTAAAACWIKAAAgYfpvBy88vFf6qb9Ja3F0fqFyZIyHk1h/1Ys2qbvUyWgYz+7fjZ0f38pm7pbWWlc5z87+H9OjcVx88Chmd/plMyMitrpfUbpAEbEy2KgeGWfPXynd2+FoFKunF2NS+DXX8WgQ21+/RBT+oz97vXi7thbdVN1zw/1z++LYkcNl8yIiBjMzceP5y9K9vXTtVvHOjmNl8WTpzg7729H7tB5dV/fT9re24vOTx6U7O79wKM6sLpfNi4jYfPMhrj59Ubqzl6/fKd3ZwWgc55ZPle5sf2c7Pm68L93ZuUk/jvc3oxXe26cOHIzZE0tl8yIivr9+F0s3b//xIr2ZAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJAgpgAAABLEFAAAQIKYAgAASBBTAAAACWIKAAAgQUwBAAAkiCkAAIAEMQUAAJDQtdZ2+xwAAAD2HG+mAAAAEsQUAABAgpgCAABIEFMAAAAJYgoAACBBTAEAACT8Bk++f0O7IvBDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_batch = minibatch(valAB, 1)\n",
    "for idx in range(len(valAB[:10])):  # maximum 10 loop\n",
    "    print(f'Processing palette {idx+1} ... ')\n",
    "    \n",
    "    # get fake & real array from minibatch\n",
    "    _, valA, valB = next(val_batch)\n",
    "    \n",
    "    # transform normalized array(for model input) into pixel array\n",
    "    im_fake = ((valA+1)/2*255).clip(0,255).astype('uint8')[0]\n",
    "    im = ((valB+1)/2*255).clip(0,255).astype('uint8')[0]\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    # visualize original palette & concealed palette\n",
    "    plt.subplot(1,5,1)\n",
    "    plt.imshow(np.array(Image.fromarray(im_fake).resize((512,80))))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,5,2)\n",
    "    plt.imshow(np.array(Image.fromarray(im).resize((512,80))))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # predict concealed colors\n",
    "    result = generator.predict(valA)[0]\n",
    "    result1 = generator1.predict(valA)[0]\n",
    "    result2 = generator2.predict(valA)[0]\n",
    "    \n",
    "    batch = int(256 / 5)\n",
    "    for i in range(5):\n",
    "        result[:,i*batch:(i+1)*batch,:] = np.mean(result[:,i*batch:(i+1)*batch,:],1)[0]\n",
    "        result1[:,i*batch:(i+1)*batch,:] = np.mean(result1[:,i*batch:(i+1)*batch,:],1)[0]\n",
    "        result2[:,i*batch:(i+1)*batch,:] = np.mean(result2[:,i*batch:(i+1)*batch,:],1)[0]\n",
    "    \n",
    "    # visualize predicted colors\n",
    "    plt.subplot(1,5,3)\n",
    "    res = np.array(Image.fromarray(((result+1)/2*255).clip(0,255).astype('uint8')).resize((512,80)))\n",
    "    plt.imshow(res)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,5,4)\n",
    "    res1 = np.array(Image.fromarray(((result1+1)/2*255).clip(0,255).astype('uint8')).resize((512,80)))\n",
    "    plt.imshow(res1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,5,5)\n",
    "    res2 = np.array(Image.fromarray(((result2+1)/2*255).clip(0,255).astype('uint8')).resize((512,80)))\n",
    "    plt.imshow(res2)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
